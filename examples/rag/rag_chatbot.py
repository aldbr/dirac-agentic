"""RAG chatbot using the DIRAC knowledge base.

A self-contained chatbot that:
1. Loads a HuggingFace dataset (generated by dirac-dataset)
2. Embeds and indexes it into a local Milvus vector DB
3. Answers questions using semantic search + HuggingFace Inference API

Configuration via environment variables:
    HF_TOKEN: HuggingFace API token (required for gated models / higher rate limits)
    HF_MODEL: Model to use (default: meta-llama/Meta-Llama-3.1-8B-Instruct)

Usage:
    pip install -r requirements.txt

    # Build from a local HuggingFace dataset (generated by dirac-dataset)
    python rag_chatbot.py build /path/to/hf_dataset

    # Or build directly from HuggingFace Hub
    python rag_chatbot.py build myorg/dirac-docs

    # Then: chat against the vector DB
    python rag_chatbot.py chat
"""

import sys
from pathlib import Path

from datasets import DatasetDict, load_dataset
from huggingface_hub import InferenceClient
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from pymilvus import CollectionSchema, DataType, FieldSchema, MilvusClient
from pymilvus.milvus_client import IndexParams
from rich.console import Console
from rich.markdown import Markdown

# Defaults
DB_PATH = "./dirac_vector.db"
COLLECTION = "doc_embeddings"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"

console = Console()


# ---------------------------------------------------------------------------
# Build: HF dataset -> Milvus vector DB
# ---------------------------------------------------------------------------


def _load_dataset(dataset_ref: str) -> DatasetDict:
    """Load a dataset from a local path or HuggingFace Hub.

    If dataset_ref is an existing directory, loads from disk.
    Otherwise, treats it as a HuggingFace Hub repo ID.
    """
    if Path(dataset_ref).is_dir():
        console.print(f"Loading dataset from disk: [cyan]{dataset_ref}[/cyan]")
        return DatasetDict.load_from_disk(dataset_ref)

    console.print(f"Loading dataset from HuggingFace Hub: [cyan]{dataset_ref}[/cyan]")
    ds = load_dataset(dataset_ref)
    if not isinstance(ds, DatasetDict):
        ds = DatasetDict({"train": ds})
    return ds


def build_index(dataset_ref: str, db_path: str = DB_PATH) -> None:
    """Load a HuggingFace dataset and index it into Milvus.

    Args:
        dataset_ref: Local path to a HF dataset, or a HuggingFace Hub repo ID.
        db_path: Path to the Milvus Lite database file.
    """
    ds = _load_dataset(dataset_ref)

    console.print(f"Loading embedding model [cyan]{EMBEDDING_MODEL}[/cyan]")
    embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)

    client = MilvusClient(db_path)

    for split_name, split_ds in ds.items():
        if len(split_ds) == 0:
            continue

        console.print(f"Embedding [green]{split_name}[/green]: {len(split_ds)} documents")
        texts = [r["text"] for r in split_ds]

        # Embed in batches
        embeddings = embed_model.get_text_embedding_batch(texts)

        # Create collection if needed
        if not client.has_collection(COLLECTION):
            fields = [
                FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
                FieldSchema(
                    name="embedding",
                    dtype=DataType.FLOAT_VECTOR,
                    dim=len(embeddings[0]),
                ),
                FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=2048),
                FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=128),
            ]
            schema = CollectionSchema(fields, description="DIRAC document embeddings")
            client.create_collection(collection_name=COLLECTION, schema=schema)

        # Insert
        data = [
            {"embedding": emb, "text": txt, "source": split_name}
            for emb, txt in zip(embeddings, texts)
        ]
        client.insert(collection_name=COLLECTION, data=data)
        client.flush(collection_name=COLLECTION)
        console.print(f"  Inserted {len(data)} records")

    # Create index
    index_params = IndexParams()
    index_params.add_index(field_name="embedding", index_type="AUTOINDEX", metric_type="COSINE")
    client.create_index(collection_name=COLLECTION, index_params=index_params)
    client.load_collection(collection_name=COLLECTION)

    console.print(f"\n[bold green]Vector DB built at[/bold green] {db_path}")


# ---------------------------------------------------------------------------
# Search: query the vector DB
# ---------------------------------------------------------------------------


def search(query: str, db_path: str = DB_PATH, top_k: int = 5) -> list[dict]:
    """Search the vector DB for documents similar to the query."""
    embed_model = HuggingFaceEmbedding(model_name=EMBEDDING_MODEL)
    query_embedding = embed_model.get_text_embedding(query)

    client = MilvusClient(db_path)
    if not client.has_collection(COLLECTION):
        return []

    results = client.search(
        collection_name=COLLECTION,
        data=[query_embedding],
        limit=top_k,
        search_params={"metric_type": "COSINE"},
        output_fields=["text", "source"],
    )

    return [
        {
            "text": r["entity"]["text"],
            "source": r["entity"]["source"],
            "score": r["distance"],
        }
        for r in (results[0] if results else [])
    ]


# ---------------------------------------------------------------------------
# Chat: interactive RAG loop
# ---------------------------------------------------------------------------


def chat(db_path: str = DB_PATH) -> None:
    """Interactive chatbot using RAG."""
    import os

    llm_model = os.environ.get("HF_MODEL", LLM_MODEL)
    client = InferenceClient()

    console.print("[bold]DIRAC RAG Chatbot[/bold] (type 'quit' to exit)\n")

    while True:
        question = console.input("[bold cyan]You:[/bold cyan] ").strip()
        if question.lower() in ("quit", "exit", "q"):
            break
        if not question:
            continue

        # Retrieve
        with console.status("Searching knowledge base..."):
            results = search(question, db_path)

        if not results:
            console.print("[yellow]No results found. Is the vector DB built?[/yellow]\n")
            continue

        context = "\n\n---\n\n".join(
            f"[{i + 1}] Source: {r['source']} (score: {r['score']:.3f})\n{r['text']}"
            for i, r in enumerate(results)
        )

        # Generate
        with console.status("Generating answer..."):
            response = client.chat.completions.create(
                model=llm_model,
                messages=[
                    {
                        "role": "system",
                        "content": (
                            "You are a DIRAC expert assistant. Answer questions using the "
                            "provided context from DIRAC documentation and papers. "
                            "Cite sources when possible. If the context doesn't contain "
                            "enough information, say so."
                        ),
                    },
                    {
                        "role": "user",
                        "content": f"Context:\n{context}\n\nQuestion: {question}",
                    },
                ],
            )
            answer = response.choices[0].message.content or ""

        console.print()
        console.print(Markdown(answer))
        console.print()


# ---------------------------------------------------------------------------
# CLI
# ---------------------------------------------------------------------------


def main() -> None:
    if len(sys.argv) < 2:
        console.print("Usage:")
        console.print("  python rag_chatbot.py build <dataset>   # local path or HF Hub repo ID")
        console.print("  python rag_chatbot.py chat")
        return

    command = sys.argv[1]

    if command == "build":
        if len(sys.argv) < 3:
            console.print("[red]Error: provide a dataset path or HuggingFace Hub repo ID[/red]")
            console.print("  python rag_chatbot.py build ./my_dataset")
            console.print("  python rag_chatbot.py build myorg/dirac-docs")
            sys.exit(1)
        build_index(sys.argv[2])

    elif command == "chat":
        if not Path(DB_PATH).exists():
            console.print(
                f"[red]Vector DB not found at {DB_PATH}[/red]\n"
                "Build it first:\n"
                "  python rag_chatbot.py build /path/to/hf_dataset"
            )
            sys.exit(1)
        chat()

    else:
        console.print(f"[red]Unknown command: {command}[/red]")
        sys.exit(1)


if __name__ == "__main__":
    main()
